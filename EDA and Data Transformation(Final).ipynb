{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BhyVe17iu6CccAO4VFrv5dkB6f73Fh5R","timestamp":1733466104185}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLSM8NjbECS7","executionInfo":{"status":"ok","timestamp":1715622225640,"user_tz":420,"elapsed":28300,"user":{"displayName":"Namratha Sampath Kumar","userId":"07706357640781878620"}},"outputId":"19b712b8-08ca-488c-fc32-6f7213fb7df1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPmxyiW1DXm1","executionInfo":{"status":"ok","timestamp":1715622245042,"user_tz":420,"elapsed":19404,"user":{"displayName":"Namratha Sampath Kumar","userId":"07706357640781878620"}},"outputId":"f384d651-56e3-45f4-d00c-a783256691b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","source":["!pip install pyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FZtXyi-CZhE","executionInfo":{"status":"ok","timestamp":1713409000425,"user_tz":420,"elapsed":9049,"user":{"displayName":"Namratha Sampath Kumar","userId":"07706357640781878620"}},"outputId":"9c65812e-24f4-4041-e6cf-2f306cfa78cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyPDF2\n","Successfully installed pyPDF2-3.0.1\n"]}]},{"cell_type":"markdown","source":["**Load Data**"],"metadata":{"id":"NDCnHHq7rJZI"}},{"cell_type":"code","source":["import pandas as pd\n","import PyPDF2"],"metadata":{"id":"4W3m3y_S-u_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = ('/content/drive.pdf')"],"metadata":{"id":"IgOh2yz0-t65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PyPDF2\n","\n","# Path to the consolidated PDFs\n","pdf_path = '/content/drive/Shareddrives/DATA298B_Final/Data/All_PDFs_Consolidated'\n","\n","# Open the PDF file\n","with open(pdf_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    all_text = \"\"\n","    # Loop through all pages and extract text\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        page_text = page.extract_text()\n","        all_text += page_text\n","\n","# Output or process the extracted text\n","print(all_text)\n"],"metadata":{"id":"q7Uj6Fdj_KnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PyPDF2 import PdfReader\n","\n","def get_pdf_size(file_path):\n","    # Open the PDF file\n","    with open(file_path, 'rb') as file:\n","        # Create a PDF reader object\n","        pdf_reader = PdfReader(file)\n","\n","        # Initialize total size variable\n","        total_size = 0\n","\n","        # Iterate through each page of the PDF\n","        for page in pdf_reader.pages:\n","            # Get the text from the page\n","            page_text = page.extract_text()\n","\n","            # Add the size of the text on this page to the total size\n","            total_size += len(page_text)\n","\n","        # Return the total size\n","        return total_size\n","\n","# Function to calculate average PDF size\n","def average_pdf_size(directory):\n","    # Initialize variables\n","    total_size = 0\n","    num_pdfs = 0\n","\n","    # Iterate through each file in the directory\n","    for file_name in os.listdir(directory):\n","        if file_name.endswith('.pdf'):\n","            # Get the full file path\n","            file_path = os.path.join(directory, file_name)\n","\n","            # Calculate the size of the PDF and add it to the total size\n","            pdf_size = get_pdf_size(file_path)\n","            total_size += pdf_size\n","\n","            # Increment the number of PDFs\n","            num_pdfs += 1\n","\n","            # Print the PDF name and its size (optional)\n","            print(f\"{file_name}: {pdf_size} characters\")\n","\n","    # Calculate the average size\n","    if num_pdfs > 0:\n","        average_size = total_size / num_pdfs\n","        print(f\"Average PDF size (Characters): {average_size:.2f}\")\n","    else:\n","        print(\"No PDF files found in the directory.\")\n","\n","# Specify the directory containing the PDFs\n","pdf_directory = \"/content/drive/Shareddrives/DATA298B_Final/Code/Data\"\n","\n","# Call the function to calculate average PDF size\n","average_pdf_size(pdf_directory)\n"],"metadata":{"id":"nHQSlBYnDZpm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PyPDF2 import PdfReader\n","\n","def get_special_character_percentage(text):\n","    # Define special characters (you can customize this based on your definition of special characters)\n","    special_characters = \"!@#$%^&*()-_=+[]{}|;:',.<>?`~\"\n","\n","    # Initialize variables\n","    total_characters = len(text)\n","    special_character_count = sum(text.count(char) for char in special_characters)\n","\n","    # Calculate percentage of special characters\n","    if total_characters > 0:\n","        special_character_percentage = (special_character_count / total_characters) * 100\n","    else:\n","        special_character_percentage = 0\n","\n","    return special_character_percentage\n","\n","def get_pdf_special_character_percentage(file_path):\n","    # Open the PDF file\n","    with open(file_path, 'rb') as file:\n","        # Create a PDF reader object\n","        pdf_reader = PdfReader(file)\n","\n","        # Initialize an empty string to store text from all pages\n","        pdf_text = \"\"\n","\n","        # Iterate through each page of the PDF\n","        for page in pdf_reader.pages:\n","            # Get the text from the page and append it to pdf_text\n","            pdf_text += page.extract_text()\n","\n","        # Calculate special character percentage for the text\n","        return get_special_character_percentage(pdf_text)\n","\n","# Function to calculate percentage of special characters for PDFs in a directory\n","def special_character_percentage(directory):\n","    # Iterate through each file in the directory\n","    for file_name in os.listdir(directory):\n","        if file_name.endswith('.pdf'):\n","            # Get the full file path\n","            file_path = os.path.join(directory, file_name)\n","\n","            # Calculate the percentage of special characters for the PDF\n","            special_char_percentage = get_pdf_special_character_percentage(file_path)\n","\n","            # Print the PDF name and its special character percentage (optional)\n","            print(f\"{file_name}: Special Character Percentage: {special_char_percentage:.2f}%\")\n","\n","# Specify the directory containing the PDFs\n","pdf_directory = \"/content/drive/Shareddrives/DATA298B_Final/Code/Data\"\n","\n","# Call the function to calculate percentage of special characters\n","special_character_percentage(pdf_directory)\n"],"metadata":{"id":"QvyG3tRsDuUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_characters = len(all_text)\n","print(\"Total number of characters in the PDF:\", total_characters)"],"metadata":{"id":"jYkFbd1-DlYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = all_text.split(' ')\n","len(words)\n","print('Number of words:', len(words))"],"metadata":{"id":"SIR0i1fZKtMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PyPDF2\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","# Define function to extract text from a PDF file\n","def extract_text_from_pdf(pdf_path):\n","    with open(pdf_path, 'rb') as pdf_file:\n","        pdf_reader = PyPDF2.PdfReader(pdf_file)\n","        all_text = \"\"\n","        for page in pdf_reader.pages:\n","            all_text += page.extract_text()\n","    return all_text\n","\n","# Define function to plot word frequency\n","def plot_word_frequency(word_counts, top_n=20):\n","    # Extract the most common words and their counts\n","    words_to_plot, counts_to_plot = zip(*word_counts.most_common(top_n))\n","\n","    # Create a bar chart\n","    plt.figure(figsize=(12, 8))\n","    plt.bar(words_to_plot, counts_to_plot, color='skyblue')\n","    plt.xlabel('Words', fontsize=14)\n","    plt.ylabel('Frequency', fontsize=14)\n","    plt.title('Top Word Frequency Distribution', fontsize=16)\n","    plt.xticks(rotation=45, fontsize=12)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Path to the PDF file\n","pdf_path = '/content/drive/Shareddrives/DATA298B_Final/Data/All_PDFs_Consolidated'\n","\n","# Extract text from the PDF\n","all_text = extract_text_from_pdf(pdf_path)\n","\n","# Tokenize the text by splitting on whitespace\n","words = all_text.split()\n","\n","# Calculate word frequencies\n","word_counts = Counter(words)\n","\n","# Plot the word frequency distribution for the top 20 words\n","plot_word_frequency(word_counts, top_n=20)\n"],"metadata":{"id":"y3knBjytmMoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re# Function to count special characters\n","def count_special_characters(text):\n","    special_characters = re.findall(r'[^\\w\\s]', text)\n","    return len(special_characters)\n","\n","# Function to count URLs\n","def count_urls(text):\n","    urls = re.findall(r'https?:\\/\\/.[\\r\\n]', text)\n","    return len(urls)\n","\n","# Function to count HTML link tags\n","def count_html_links(text):\n","    html_links = re.findall(r'\\<a href', text)\n","    return len(html_links)\n","\n","# Function to count punctuation marks\n","def count_punctuation_marks(text):\n","    punctuation_marks = re.findall(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', text)\n","    return len(punctuation_marks)\n","\n","# Function to count HTML line break tags\n","def count_html_line_breaks(text):\n","    html_line_breaks = re.findall(r'<br />', text)\n","    return len(html_line_breaks)\n","\n","# Function to count single quotation marks\n","def count_single_quotation_marks(text):\n","    single_quotation_marks = re.findall(r'\\'', text)\n","    return len(single_quotation_marks)\n","\n","# Count occurrences of each pattern\n","special_characters_count = count_special_characters(all_text)\n","urls_count = count_urls(all_text)\n","html_links_count = count_html_links(all_text)\n","punctuation_marks_count = count_punctuation_marks(all_text)\n","html_line_breaks_count = count_html_line_breaks(all_text)\n","single_quotation_marks_count = count_single_quotation_marks(all_text)\n","\n","# Printing the counts\n","print(\"Special Characters Count:\", special_characters_count)\n","print(\"URLs Count:\", urls_count)\n","print(\"HTML Links Count:\", html_links_count)\n","print(\"Punctuation Marks Count:\", punctuation_marks_count)\n","print(\"HTML Line Breaks Count:\", html_line_breaks_count)\n","print(\"Single Quotation Marks Count:\", single_quotation_marks_count)"],"metadata":{"id":"WQHkfZc1ozAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import matplotlib.pyplot as plt\n","\n","# Extracting all special characters from the text\n","special_characters = re.findall(r'[^a-zA-Z0-9\\s]', all_text)\n","\n","# Counting the frequency of each special character\n","character_counts = {}\n","for char in special_characters:\n","    if char in character_counts:\n","        character_counts[char] += 1\n","    else:\n","        character_counts[char] = 1\n","\n","# Sorting the characters by frequency\n","sorted_characters = sorted(character_counts.items(), key=lambda x: x[1], reverse=True)\n","\n","# Selecting the top 10 special characters\n","top_characters = sorted_characters[:10]\n","chars, counts = zip(*top_characters)\n","\n","# Printing descriptive results\n","print(\"Top 10 Special Characters:\")\n","for char, count in top_characters:\n","    print(f\"{char}: {count}\")\n","\n","# Creating the bar chart\n","plt.figure(figsize=(10, 6))\n","plt.bar(chars, counts)\n","plt.xlabel('Special Characters')\n","plt.ylabel('Frequency')\n","plt.title('Top 10 Special Characters')\n","plt.xticks(rotation=45)\n","plt.grid(True)  # Add gridlines for better readability\n","plt.tight_layout()  # Adjust layout to prevent clipping of labels\n","plt.show()\n"],"metadata":{"id":"OPhXWf8xkOzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PyPDF2\n","\n","def calculate_special_character_percentage(text):\n","    special_characters = \"!@#$%^&*()-_+=[]{}|:;'<>,.?/~\"\n","    special_count = 0\n","    total_count = 0\n","\n","    for char in text:\n","        total_count += 1\n","        if char in special_characters:\n","            special_count += 1\n","\n","    if total_count > 0:\n","        special_percentage = (special_count / total_count) * 100\n","    else:\n","        special_percentage = 0\n","\n","    return special_percentage\n","\n","# Open the PDF file\n","with open('/content/drive/Shareddrives/DATA298B_Final/Data/All_PDFs_Consolidated', 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    all_text = \"\"\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        page_text = page.extract_text()\n","        all_text += page_text\n","\n","# Calculate the percentage of special characters\n","percentage_special = calculate_special_character_percentage(all_text)\n","print(f\"Percentage of special characters: {percentage_special:.2f}%\")\n"],"metadata":{"id":"RBeqQN0cGvpn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n","import matplotlib.pyplot as plt"],"metadata":{"id":"Wxcw7UTSY8dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#nltk.download(stopwords)"],"metadata":{"id":"u2wTgULfZtoJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Preparing Input Data for Analysis**"],"metadata":{"id":"_mYu0OOZo7ov"}},{"cell_type":"code","source":["def preprocess_input(text):\n","    ## clean\n","    text = re.sub(r'[^\\w\\s]', '', str(text).strip())\n","    text = re.sub(r'https?:\\/\\/.[\\r\\n]', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\<a href', ' ', text)\n","    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'\\'', ' ', text)\n","\n","    return text"],"metadata":{"id":"zhGND6KNaSHp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["viz_txt = preprocess_input(all_text)"],"metadata":{"id":"wm_wzM5DblKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to count occurrences of special characters\n","def count_special_characters(text):\n","    special_characters = re.findall(r'[^\\w\\s]', text)\n","    return len(special_characters)\n","\n","# Function to count URLs\n","def count_urls(text):\n","    urls = re.findall(r'https?:\\/\\/.[\\r\\n]', text)\n","    return len(urls)\n","\n","# Function to count HTML link tags\n","def count_html_links(text):\n","    html_links = re.findall(r'\\<a href', text)\n","    return len(html_links)\n","\n","# Function to count punctuation marks\n","def count_punctuation_marks(text):\n","    punctuation_marks = re.findall(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', text)\n","    return len(punctuation_marks)\n","\n","# Function to count HTML line break tags\n","def count_html_line_breaks(text):\n","    html_line_breaks = re.findall(r'<br />', text)\n","    return len(html_line_breaks)\n","\n","# Function to count single quotation marks\n","def count_single_quotation_marks(text):\n","    single_quotation_marks = re.findall(r'\\'', text)\n","    return len(single_quotation_marks)\n","\n","# Count occurrences of each pattern in the preprocessed text\n","special_characters_count = count_special_characters(viz_txt)\n","urls_count = count_urls(viz_txt)\n","html_links_count = count_html_links(viz_txt)\n","punctuation_marks_count = count_punctuation_marks(viz_txt)\n","html_line_breaks_count = count_html_line_breaks(viz_txt)\n","single_quotation_marks_count = count_single_quotation_marks(viz_txt)\n","\n","# Printing the counts\n","print(\"Special Characters Count:\", special_characters_count)\n","print(\"URLs Count:\", urls_count)\n","print(\"HTML Links Count:\", html_links_count)\n","print(\"Punctuation Marks Count:\", punctuation_marks_count)\n","print(\"HTML Line Breaks Count:\", html_line_breaks_count)\n","print(\"Single Quotation Marks Count:\", single_quotation_marks_count)"],"metadata":{"id":"jDTpLZ7vq4Zc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Download the stopwords from NLTK\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HG--XQBMcCho","executionInfo":{"status":"ok","timestamp":1713409108734,"user_tz":420,"elapsed":1108,"user":{"displayName":"Namratha Sampath Kumar","userId":"07706357640781878620"}},"outputId":"a9862326-945c-4f69-fe01-0ab76e7a64c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["tokens = word_tokenize(viz_txt.lower())  # Convert to lowercase and tokenize\n","print(tokens)\n","# Removing stopwords\n","filtered_words = [word for word in tokens if word not in stopwords.words('english')]"],"metadata":{"id":"hycPOKUldrDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_words"],"metadata":{"id":"1U2NXzcRhDDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#filtered_tokens = [word.lower() for word in filtered_words]"],"metadata":{"id":"4pdhHK9EgQDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pronouns = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n","            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n","            'theirs', 'themselves', 'l'}"],"metadata":{"id":"xUkWepC9fWJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_tokens = [token for token in filtered_words if token not in pronouns]"],"metadata":{"id":"Go8j-1lVfdoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Before preprocessing\n","print(\"Text before preprocessing:\")\n","print(all_text)\n","\n","# After preprocessing\n","print(\"\\nText after preprocessing:\")\n","print(filtered_tokens)\n"],"metadata":{"id":"yLzFZ3VtbuJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","word_counts = Counter(filtered_tokens)"],"metadata":{"id":"xNSquCPRd2O_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**EDA**"],"metadata":{"id":"_AiwHoOZpD-d"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Extracting words and their counts\n","words, counts = zip(*word_counts.most_common(20))  # You can adjust the number to display more or fewer words\n","print(words)\n","\n","# Creating the bar chart\n","plt.figure(figsize=(10, 8))\n","plt.bar(words, counts)\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')\n","plt.title('Word Frequency Distribution')\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"qJPRiDo4d8vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Assuming you already have `words` and `counts` from your previous code\n","# words, counts = zip(*word_counts.most_common(20))\n","\n","# Descriptive statistics\n","count_array = np.array(counts)\n","unique_words = len(words)\n","total_count = np.sum(count_array)\n","\n","# Print descriptive statistics\n","print(\"Descriptive Statistics of Word Frequency:\")\n","print(\"Unique Words:\", unique_words)\n","print(\"Total Count:\", total_count)\n","\n"],"metadata":{"id":"eB1lLUawGRvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","# Generate word cloud\n","wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n","\n","# Plot word cloud\n","plt.figure(figsize=(10, 6))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.title('Word Cloud')\n","plt.show()"],"metadata":{"id":"gS0MCU6zDJGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the text into sentences\n","sentences = re.split(r'[.!?]', all_text)\n","\n","# Calculate sentence lengths\n","sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n","\n","# Create a histogram of sentence lengths\n","plt.figure(figsize=(8, 6))\n","plt.hist(sentence_lengths, bins=20, color='skyblue', edgecolor='black')\n","plt.xlabel('Sentence Length')\n","plt.ylabel('Frequency')\n","plt.title('Histogram of Sentence Lengths')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"BTHnDLS1FEuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Calculate summary statistics\n","mean_length = np.mean(sentence_lengths)\n","median_length = np.median(sentence_lengths)\n","max_length = np.max(sentence_lengths)\n","min_length = np.min(sentence_lengths)\n","\n","# Print descriptive results\n","print(\"Descriptive Results:\")\n","print(f\"Mean sentence length: {mean_length:.2f} words\")\n","print(f\"Median sentence length: {median_length} words\")\n","print(f\"Maximum sentence length: {max_length} words\")\n","print(f\"Minimum sentence length: {min_length} words\")\n"],"metadata":{"id":"2X9Tg85UEqzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the text into paragraphs\n","paragraphs = all_text.split('\\n\\n')  # Assuming paragraphs are separated by double line breaks\n","\n","# Calculate paragraph lengths\n","paragraph_lengths = [len(paragraph.split()) for paragraph in paragraphs if paragraph.strip()]\n","\n","# Create a histogram of paragraph lengths\n","plt.figure(figsize=(8, 6))\n","plt.hist(paragraph_lengths, bins=20, color='skyblue', edgecolor='black')\n","plt.xlabel('Paragraph Length')\n","plt.ylabel('Frequency')\n","plt.title('Histogram of Paragraph Lengths')\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"j3pfl-DuFHy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Calculate summary statistics\n","mean_length = np.mean(paragraph_lengths)\n","median_length = np.median(paragraph_lengths)\n","max_length = np.max(paragraph_lengths)\n","min_length = np.min(paragraph_lengths)\n","\n","# Print descriptive results\n","print(\"Descriptive Results:\")\n","print(f\"Mean paragraph length: {mean_length:.2f} words\")\n","print(f\"Median paragraph length: {median_length} words\")\n","print(f\"Maximum paragraph length: {max_length} words\")\n","print(f\"Minimum paragraph length: {min_length} words\")\n"],"metadata":{"id":"-5dPjxVxE-1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate word lengths\n","word_lengths = [len(word) for word in words]\n","\n","# Create a boxplot of word lengths\n","plt.figure(figsize=(8, 6))\n","plt.boxplot(word_lengths)\n","plt.xlabel('Word Length')\n","plt.title('Boxplot of Word Lengths')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"7TwdbTNaFOFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Calculate summary statistics\n","mean_length = np.mean(word_lengths)\n","median_length = np.median(word_lengths)\n","max_length = np.max(word_lengths)\n","min_length = np.min(word_lengths)\n","q1 = np.percentile(word_lengths, 25)\n","q3 = np.percentile(word_lengths, 75)\n","\n","# Print descriptive results\n","print(\"Descriptive Results:\")\n","print(f\"Mean word length: {mean_length:.2f} characters\")\n","print(f\"Median word length: {median_length} characters\")\n","print(f\"Maximum word length: {max_length} characters\")\n","print(f\"Minimum word length: {min_length} characters\")\n","print(f\"Interquartile range (IQR): {q1} to {q3} characters\")\n"],"metadata":{"id":"MBou2lT6FQ8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Process the text with spaCy\n","doc = nlp(all_text)\n","\n","# Extract named entities and their labels\n","named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n","\n","# Count the frequency of each named entity label\n","entity_labels = [entity[1] for entity in named_entities]\n","entity_label_counts = Counter(entity_labels)\n","\n","# Create a bar chart of named entity labels\n","plt.figure(figsize=(12, 6))\n","plt.bar(entity_label_counts.keys(), entity_label_counts.values())\n","plt.xlabel('Named Entity Labels')\n","plt.ylabel('Frequency')\n","plt.title('Named Entity Recognition')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"rsR-g_R7FvvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from collections import Counter\n","import nltk\n","\n","# Download the NLTK resources (only needed once)\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(all_text)\n","\n","# Perform POS tagging\n","pos_tags = nltk.pos_tag(tokens)\n","\n","# Extract POS tags\n","pos_tag_counts = Counter(tag for word, tag in pos_tags)\n","\n","# Get the POS tag labels\n","pos_labels = list(pos_tag_counts.keys())\n","\n","# Create a bar chart of POS tag counts\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(len(pos_labels)), pos_tag_counts.values())\n","plt.xlabel('POS Tags')\n","plt.ylabel('Frequency')\n","plt.title('POS Tagging Analysis')\n","plt.xticks(range(len(pos_labels)), pos_labels, rotation=45)\n","plt.show()\n"],"metadata":{"id":"ZDCYVLEOIPZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["''' import networkx as nx\n","from itertools import combinations\n","from collections import Counter\n","\n","# Tokenize text into words\n","words = nltk.word_tokenize(all_text)\n","\n","# Generate word pairs\n","word_pairs = combinations(words, 2)\n","\n","# Count co-occurrences of word pairs\n","co_occurrences = Counter(word_pairs)\n","\n","# Create a weighted graph\n","G = nx.Graph()\n","for pair, weight in co_occurrences.items():\n","    G.add_edge(pair[0], pair[1], weight=weight)\n","\n","# Visualize the graph\n","plt.figure(figsize=(12, 8))\n","pos = nx.spring_layout(G, k=0.1)\n","nx.draw(G, pos, node_size=50, with_labels=False, alpha=0.6, edge_color='r')\n","plt.title('Text Network Visualization')\n","plt.show() '''"],"metadata":{"id":"HE6u4W2QLghV","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1713222211991,"user_tz":420,"elapsed":9,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"857916c7-41cf-478f-e742-f7ff960cce05"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" import networkx as nx\\nfrom itertools import combinations\\nfrom collections import Counter\\n\\n# Tokenize text into words\\nwords = nltk.word_tokenize(all_text)\\n\\n# Generate word pairs\\nword_pairs = combinations(words, 2)\\n\\n# Count co-occurrences of word pairs\\nco_occurrences = Counter(word_pairs)\\n\\n# Create a weighted graph\\nG = nx.Graph()\\nfor pair, weight in co_occurrences.items():\\n    G.add_edge(pair[0], pair[1], weight=weight)\\n\\n# Visualize the graph\\nplt.figure(figsize=(12, 8))\\npos = nx.spring_layout(G, k=0.1)\\nnx.draw(G, pos, node_size=50, with_labels=False, alpha=0.6, edge_color='r')\\nplt.title('Text Network Visualization')\\nplt.show() \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["print(all_text)"],"metadata":{"id":"Yr0WKJl0ELe1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pre Processing - ..continuity for preprocessing before EDA**"],"metadata":{"id":"kPDbAZoEp2oK"}},{"cell_type":"code","source":["clean_txt = all_text.lower()"],"metadata":{"id":"4Wm-hXayEYoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def remove_punctuation(text):\n","    # Pattern includes common punctuation and additional stop marks.\n","    pattern = r'[,.!?;:]'  # Add any other punctuations if needed\n","    # Replace these punctuation marks with nothing (i.e., remove them)\n","    clean_text = re.sub(pattern, '', text)\n","    return clean_text"],"metadata":{"id":"b0I7DTviG4d3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_text_1 = remove_punctuation(clean_txt)"],"metadata":{"id":"YjhuDXwgImnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["''' pattern = r\"(Page \\d+ TITLE \\d+—\\w+ AND \\w+ § \\d+[a-z]?)|(§ \\d+[a-z]? TITLE \\d+—\\w+ AND \\w+ Page \\d+)\"\n","\n","\n","# Replace the matched patterns with an empty string\n","final_txt = re.sub(pattern, \"\", clean_text_1)\n","'''\n"],"metadata":{"id":"HdCLSjX8fAVw","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1713341364595,"user_tz":420,"elapsed":3,"user":{"displayName":"Yuti Khamker","userId":"14199695670564111404"}},"outputId":"385e435a-1587-4bcc-ef0d-5f30180728d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' pattern = r\"(Page \\\\d+ TITLE \\\\d+—\\\\w+ AND \\\\w+ § \\\\d+[a-z]?)|(§ \\\\d+[a-z]? TITLE \\\\d+—\\\\w+ AND \\\\w+ Page \\\\d+)\"\\n\\n\\n# Replace the matched patterns with an empty string\\nfinal_txt = re.sub(pattern, \"\", clean_text_1)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["print(final_txt)"],"metadata":{"id":"cdhT-lgMLKHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming final_txt contains the processed text\n","print(\"Final Text:\")\n","print(final_txt)\n","\n","# Calculate total number of characters\n","total_characters = len(final_txt)\n","print(f\"Total number of characters: {total_characters}\")\n"],"metadata":{"id":"ViYGDYFQKYoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming final_txt contains the processed text\n","words = final_txt.split()\n","num_words = len(words)\n","print(f\"Number of words: {num_words}\")\n"],"metadata":{"id":"bc6MbNwYKs-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Transformation**\n","\n","**Synonym Replacement**"],"metadata":{"id":"R5vRJhYFeCl_"}},{"cell_type":"code","source":["# Define the start and end patterns\n","start_pattern = r\"subchapter i—federal trade commission\"\n","end_pattern = r\"any commissioner may be re -\"\n","\n","# Find the start and end indices of the text to extract\n","start_index = final_txt.find(start_pattern) + len(start_pattern)\n","end_index = final_txt.find(end_pattern, start_index)\n","\n","# Extract the text\n","extracted_text = final_txt[start_index:end_index].strip()\n","\n","print(extracted_text)"],"metadata":{"id":"Eg3xB3QnS1yN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the original and synonym-replaced text\n","print(\"Original Text:\")\n","\n","print(extracted_text)"],"metadata":{"id":"rQ0e25IwglV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","from nltk.tokenize import sent_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","# Tokenize the text into sentences\n","sentences = sent_tokenize(extracted_text)\n","\n","# Find synonyms for each word in the tokens\n","synonym_sentences = []\n","for sentence in sentences:\n","    tokens = word_tokenize(sentence)\n","    tagged_tokens = pos_tag(tokens)\n","    synonyms = []\n","    for word, tag in tagged_tokens:\n","        if tag.startswith('NN') or tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('RB'):\n","            synsets = wordnet.synsets(word)\n","            if synsets:\n","                synonyms.append(synsets[0].lemmas()[0].name())\n","            else:\n","                synonyms.append(word)\n","        else:\n","            synonyms.append(word)\n","    synonym_sentence = ' '.join(synonyms)\n","    synonym_sentences.append(synonym_sentence)\n","\n","# Join the synonym sentences back into a paragraph\n","synonym_text = ' '.join(synonym_sentences)\n","\n","print(\"\\nSynonym-Replaced Text:\")\n","# Split the string into chunks of 80 characters\n","chunk_size = 80\n","chunks = [synonym_text[i:i+chunk_size] for i in range(0, len(synonym_text), chunk_size)]\n","\n","# Print each chunk on a new line\n","for chunk in chunks:\n","    print(chunk)\n"],"metadata":{"id":"-G3G-ARHZyUA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Word Deletion: Delete random words to sentences.**"],"metadata":{"id":"Z33LRXNlgwRr"}},{"cell_type":"code","source":["pip install nlpaug\n"],"metadata":{"id":"wU2RLjZLgw-f","executionInfo":{"status":"ok","timestamp":1713341395407,"user_tz":420,"elapsed":7249,"user":{"displayName":"Yuti Khamker","userId":"14199695670564111404"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"de743643-78a2-4e5a-c885-c3f7bdffee85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.7.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}]},{"cell_type":"code","source":["import textwrap\n","import nlpaug.augmenter.word as naw\n","\n","# Initialize the augmenter for word deletion\n","aug = naw.RandomWordAug(action=\"delete\", aug_p=0.2)\n","\n","# Perform word deletion on the text\n","deleted_texts = aug.augment(extracted_text)\n","\n","# Join the list of deleted texts into a single string\n","deleted_text = ' '.join(deleted_texts)\n","\n","# Wrap the deleted text\n","wrapped_deleted_text = textwrap.fill(deleted_text, width=80)\n","\n","print(\"\\nWord-Deleted Text:\")\n","print(wrapped_deleted_text)\n"],"metadata":{"id":"ludpWaEtg0Rw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Word Swap**"],"metadata":{"id":"zwrUlIOdg1XP"}},{"cell_type":"code","source":["import textwrap\n","import nlpaug.augmenter.word as naw\n","\n","# Initialize the augmenter for word swap\n","aug = naw.RandomWordAug(action=\"swap\")\n","\n","# Perform word swap on the text\n","swapped_texts = aug.augment(extracted_text)\n","\n","# Join the list of swapped texts into a single string\n","swapped_text = ' '.join(swapped_texts)\n","\n","# Wrap the swapped text\n","wrapped_swapped_text = textwrap.fill(swapped_text, width=80)\n","\n","\n","print(\"\\nWord-Swapped Text:\")\n","print(wrapped_swapped_text)\n"],"metadata":{"id":"VZWfC4bJg4w8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"id":"Oe6APbScg87k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Initialize the WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Tokenize the sentence into words\n","words = nltk.word_tokenize(sentence)\n","\n","# Lemmatize each word in the sentence\n","lemmatized_sentence = ' '.join([lemmatizer.lemmatize(word) for word in words])\n","\n","print(\"Original sentence:\", sentence)\n","print(\"Lemmatized sentence:\", lemmatized_sentence)\n"],"metadata":{"id":"0c26i08yhAIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Tokenization**"],"metadata":{"id":"szygHN_LhlYd"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# Tokenize the text into words\n","words = word_tokenize(clean_txt)\n","\n","# Tokenize the text into sentences\n","sentences = sent_tokenize(clean_txt)\n","\n","# Print the tokens\n","print(\"Words:\")\n","print(words)\n"],"metadata":{"id":"--7P9h9zhqtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vectorization**"],"metadata":{"id":"m8bATGkXhySk"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","\n","# Tokenize the text into words\n","words = word_tokenize(clean_txt)\n","\n","# Create a Word2Vec model\n","model = Word2Vec([words], vector_size=100, window=5, min_count=1, sg=0)\n","\n","# Convert tokens to vectors\n","vectors = [model.wv[word] for word in words]\n","\n","# Print the vectors\n","print(\"Vectors:\")\n","for word, vector in zip(words, vectors):\n","    print(f\"{word}: {vector}\")\n"],"metadata":{"id":"yZw-hl_fhxJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Install the required dependencies to establish connection**"],"metadata":{"id":"BFwV-QFxrvAL"}},{"cell_type":"code","source":["!pip install -q cassio datasets langchain openai tiktoken"],"metadata":{"id":"AwbSyuK-JuoO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417013917,"user_tz":420,"elapsed":21190,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"04da27d7-b8c2-4488-9772-7e92bf876833"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.9/309.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# LangChain components to use\n","from langchain.vectorstores.cassandra import Cassandra\n","from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n","from langchain.llms import OpenAI\n","from langchain.embeddings import OpenAIEmbeddings\n","\n","# Support for dataset retrieval with Hugging Face\n","from datasets import load_dataset\n","\n","# With CassIO, the engine powering the Astra DB integration in LangChain,\n","# you will also initialize the DB connection:\n","import cassio"],"metadata":{"id":"WpfPVGvBJvwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install PyPDF2"],"metadata":{"id":"u-T5wCj5Jyfx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417050305,"user_tz":420,"elapsed":7384,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"3053f5f4-7279-4811-f64e-c5b2a2167d6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/232.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","source":["from PyPDF2 import PdfReader"],"metadata":{"id":"fhZUfTu3J4Yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:guszcnpFopLAlWUUHktYWmcF:7670ccb9c5af32382c4e1793acbc3b3abf487e0e74b92552c4f61399d1df5b9b\" # enter the \"AstraCS:...\" string found in in your Token JSON file\n","ASTRA_DB_ID = \"f729c71a-3e25-4a0f-8ee6-14643edb0949\" # enter your Database ID\n","\n","OPENAI_API_KEY = \"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\" # enter your OpenAI key"],"metadata":{"id":"Fwk9WsE2J8j3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pdfreader = PdfReader('/content/chap2.pdf')"],"metadata":{"id":"a_jfjWgOOefU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import Concatenate\n","# read text from pdf\n","raw_text = ''\n","for i, page in enumerate(pdfreader.pages):\n","    content = page.extract_text()\n","    if content:\n","        raw_text += content"],"metadata":{"id":"XBn1Vls3RObp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Initialize the connection to your database:**"],"metadata":{"id":"ctOTXnC-QK-x"}},{"cell_type":"code","source":["cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"],"metadata":{"id":"yRVzuilrQLtf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417075830,"user_tz":420,"elapsed":1266,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"ee0954e8-ca3f-40cd-8f81-54e35f9f91ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for f729c71a-3e25-4a0f-8ee6-14643edb0949-us-east1.db.astra.datastax.com:29042:d4c46d18-b77d-4730-b04c-86c3465b9bb5. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n","WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for f729c71a-3e25-4a0f-8ee6-14643edb0949-us-east1.db.astra.datastax.com:29042:d4c46d18-b77d-4730-b04c-86c3465b9bb5. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n","ERROR:cassandra.connection:Closing connection <LibevConnection(136944248222672) f729c71a-3e25-4a0f-8ee6-14643edb0949-us-east1.db.astra.datastax.com:29042:d4c46d18-b77d-4730-b04c-86c3465b9bb5> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n","WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for f729c71a-3e25-4a0f-8ee6-14643edb0949-us-east1.db.astra.datastax.com:29042:d4c46d18-b77d-4730-b04c-86c3465b9bb5. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"]}]},{"cell_type":"markdown","source":["**Create the LangChain embedding and LLM objects for later usage:**"],"metadata":{"id":"THZlazpWQhbU"}},{"cell_type":"code","source":["llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n","embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"],"metadata":{"id":"eZk3TGF7QeJz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417078144,"user_tz":420,"elapsed":1479,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"6e71e71d-7a38-42d7-bcce-65dbf8c4bd71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n","  warn_deprecated(\n","/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n","  warn_deprecated(\n"]}]},{"cell_type":"markdown","source":["**Create your LangChain vector store**"],"metadata":{"id":"wOXvvwMHQmbf"}},{"cell_type":"code","source":["astra_vector_store = Cassandra(\n","    embedding=embedding,\n","    table_name=\"law_chap2\",\n","    session=None,\n","    keyspace=None,\n",")\n","print(\"Table Chapter 2 created succeffully\")"],"metadata":{"id":"MZpvil8nQqUE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417081289,"user_tz":420,"elapsed":2987,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"5bbf7f78-71b5-4d6a-d9ee-9f4d42d33d1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Table Chapter 2 created succeffully\n"]}]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","# We need to split the text using Character Text Split such that it should not increse token size\n","text_splitter = CharacterTextSplitter(\n","    separator = \"\\n\",\n","    chunk_size = 512,\n","    chunk_overlap  = 150,\n","    length_function = len,\n",")\n","texts = text_splitter.split_text(raw_text)\n"],"metadata":{"id":"jawCQkWlRUjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Load the dataset into the vector store**"],"metadata":{"id":"z7exR8CCS4xs"}},{"cell_type":"code","source":["astra_vector_store.add_texts(texts)\n","\n","print(\"Number of headline inserted in AstraDB\",  len(texts))\n","\n","astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"],"metadata":{"id":"KQtEE71DSzTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(texts)"],"metadata":{"id":"uFZqpgmlUIgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import textwrap\n","\n","# Join the list of texts into a single string\n","full_text = \"\\n\".join(texts)\n","\n","# Wrap the text\n","stored_text = textwrap.fill(full_text, width=180)\n","print(stored_text)\n"],"metadata":{"id":"7_ElSwD9wUdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_question = True\n","while True:\n","    if first_question:\n","        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n","    else:\n","        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n","\n","    if query_text.lower() == \"quit\":\n","        break\n","\n","    if query_text == \"\":\n","        continue\n","\n","    first_question = False\n","\n","    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n","    answer = astra_vector_index.query(query_text, llm=llm).strip()\n","    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n","\n","    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n","    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n","        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))#showcases only the first 84 characters of the document's content"],"metadata":{"id":"2tlV0XF9b_fW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713417186643,"user_tz":420,"elapsed":72466,"user":{"displayName":"Swetha Neha Kutty Sivakumar","userId":"06411390725172794039"}},"outputId":"ebb5b9e6-ff25-49c4-8a89-c1abdb039aa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Enter your question (or type 'quit' to exit): What is the purpose of the Federal Trade Commission as established in the search results?\n","\n","QUESTION: \"What is the purpose of the Federal Trade Commission as established in the search results?\"\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n","WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"name":"stdout","output_type":"stream","text":["ANSWER: \"The purpose of the Federal Trade Commission is to grant the necessary authority to the commission to enforce laws and protect the public interest, specifically in the petroleum industry and other major investigations. The commission is authorized to directly enforce subpoenas and seek preliminary injunctive relief to prevent unfair business practices. The commission also has the power to prevent individuals from violating the provisions of the law and can refer findings and recommendations to the Attorney General for further action.\"\n","\n","FIRST DOCUMENTS BY RELEVANCE:\n","    [0.9261] \"troleum industry, as well as in other major investiga -\n","tions designed to protect th ...\"\n","    [0.9215] \"he may deem proper. \n","For the purpose of enforcing these provisions \n","the Federal Trad ...\"\n","    [0.9214] \"and procedure provided for in the Federal Trade \n","Commission Act. \n","The Commission is  ...\"\n","    [0.9210] \"business, in order that it may thereafter main -\n","tain its organization and managemen ...\"\n","\n","What's your next question (or type 'quit' to exit): How are the members of the Federal Trade Commission appointed and what are the requirements for their selection?\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"output_type":"stream","name":"stdout","text":["\n","QUESTION: \"How are the members of the Federal Trade Commission appointed and what are the requirements for their selection?\"\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"output_type":"stream","name":"stdout","text":["ANSWER: \"The members of the Federal Trade Commission are appointed by the President with the advice and consent of the Senate. Not more than three of the Commissioners can be members of the same political party.\"\n","\n","FIRST DOCUMENTS BY RELEVANCE:\n","    [0.9335] \"COMMISSION \n","§ 41. Federal Trade Commission established; \n","membership; vacancies; seal ...\"\n","    [0.9187] \"The Commission may, by one or more of its \n","members, or by such examiners as it may d ...\"\n","    [0.9165] \"ate and the House of Representatives in Congress as -\n","sembled, March 13, 1950, pursu ...\"\n","    [0.9152] \"Federal Trade Commission Act [15 U.S.C. 41 et \n","seq.]. \n","(Aug. 8, 1951, ch. 298, §10,  ...\"\n","\n","What's your next question (or type 'quit' to exit): What are the key subchapters and sections that define the authority and activities of the Federal Trade Commission?\n","\n","QUESTION: \"What are the key subchapters and sections that define the authority and activities of the Federal Trade Commission?\"\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n","WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"]},{"name":"stdout","output_type":"stream","text":["ANSWER: \"The key subchapters and sections that define the authority and activities of the Federal Trade Commission are subchapter I, section 41 et seq., and section 70e.\"\n","\n","FIRST DOCUMENTS BY RELEVANCE:\n","    [0.9384] \"Federal Trade Commission Act which comprises this \n","subchapter.\n","Statutory Notes and R ...\"\n","    [0.9322] \"Editorial Notes \n","REFERENCES IN TEXT \n","The Federal Trade Commission Act, referred to i ...\"\n","    [0.9310] \"Federal Trade Commission Act [15 U.S.C. 41 et \n","seq.]. \n","(Aug. 8, 1951, ch. 298, §10,  ...\"\n","    [0.9303] \"subsec. (c), is act Sept. 26, 1914, ch. 311, 38 Stat. 717, \n","which is classified gene ...\"\n","\n","What's your next question (or type 'quit' to exit): quit\n"]}]}]}